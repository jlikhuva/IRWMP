{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import db\n",
    "import wrangling as wr\n",
    "import numpy as np\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "# from __future__ import statistcs as stats\n",
    "\n",
    "databaseName = \"database.csv\"\n",
    "fundedProjectsFile = \"FundedProjects.csv\"\n",
    "dbreaderHandle = db.createReader(databaseName)\n",
    "fundedProjectsHandle = db.createReader(fundedProjectsFile)\n",
    "dbreader = dbreaderHandle[0]\n",
    "fprojectsreader = fundedProjectsHandle[0]\n",
    "\n",
    "# vectors & dicts\n",
    "locationLatAndLong = []\n",
    "\n",
    "countiesDict = {}\n",
    "sponsorsOfProposedProjects = {}\n",
    "areasOfInterestByCounty = {}\n",
    "\n",
    "projFundingAmounts =  []\n",
    "projTotalCosts = []\n",
    "fundedProjectsTitles = []\n",
    "\n",
    "# Vectors used during classification.\n",
    "proposedProjectsTitles = []\n",
    "proposedProjectsAbstracts = []\n",
    "proposedProjectsLocationDescriptions = []\n",
    "proposedProjectsProjectTypeDescription = []\n",
    "proposedProjectsDetailedDescription = []\n",
    "proposedProjectsProjectNeed = []\n",
    "proposedProjectsCriticalImpacts = []\n",
    "proposedProjectsBenefits = []\n",
    "\n",
    "essence = []\n",
    "\n",
    "# Bitvectors\n",
    "drinkingWater = []\n",
    "waterQualityImprovement = []\n",
    "waterReuseAndRecycling = []\n",
    "stormwaterImprovements = []\n",
    "groundwaterBenefits = []\n",
    "infiltration = []\n",
    "habitatProtection = []\n",
    "floodProtection = []\n",
    "\n",
    "#Constants\n",
    "areasOfInterest = [\n",
    "    'Drinking Water Supply',\n",
    "    'Water Quality Improvement',\n",
    "    'Water Reuse/Recycling',\n",
    "    'Stormwater Improvements',\n",
    "    'Groundwater Benefits',\n",
    "    'Infiltration',\n",
    "    'Habitat Protection and Restoration',\n",
    "    'Flood Protection'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fillSponsorsDict(row):\n",
    "    sponsorsStr = row[12]\n",
    "    sponsorsStr = sponsorsStr.strip().replace(\"\\n\", \"/\")\n",
    "    if len(sponsorsStr) == 0:\n",
    "        return\n",
    "    tokens = sponsorsStr.split(\"/\")\n",
    "    for eachAgency in tokens:\n",
    "        eachAgency = eachAgency.strip()\n",
    "        if len(sponsorsStr) == 0:\n",
    "            continue\n",
    "        if \"zone 7\" in sponsorsStr or sponsorsStr is \"zone 7\":\n",
    "            sponsorsStr = \"zone 7 water agency\"\n",
    "            \n",
    "        if eachAgency in sponsorsOfProposedProjects:\n",
    "            sponsorsOfProposedProjects[eachAgency] += 1\n",
    "        else:\n",
    "            sponsorsOfProposedProjects[eachAgency] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def appendBitvectors(row):\n",
    "    appendToV(row, drinkingWater, 13)\n",
    "    appendToV(row, waterQualityImprovement, 14)\n",
    "    appendToV(row, waterReuseAndRecycling, 15)\n",
    "    appendToV(row, stormwaterImprovements, 16)\n",
    "    appendToV(row, groundwaterBenefits, 17)\n",
    "    appendToV(row, infiltration, 18)\n",
    "    appendToV(row, habitatProtection, 19)\n",
    "    appendToV(row, floodProtection, 20)\n",
    "    \n",
    "def appendToV(row, vector, index):\n",
    "    if(\"1\" in row[index]):\n",
    "        vector.append(1)\n",
    "    else:\n",
    "        vector.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def incrementAreasOfInterestCounts(ls, row):\n",
    "    start = 13\n",
    "    end = 20         \n",
    "    for i in range(start, end+1):\n",
    "        if(\"1\" in row[i]):\n",
    "            ls[i - start] += 1\n",
    "\n",
    "def initialize(row):\n",
    "    ls = []\n",
    "    start = 13\n",
    "    end = 20\n",
    "    for i in range(start, end+1):\n",
    "        if(\"1\" in row[i]):\n",
    "            ls.append(1)\n",
    "        else:\n",
    "            ls.append(0)\n",
    "    return ls\n",
    "\n",
    "\n",
    "def appendCounties(row):\n",
    "    countyStr = row[2]\n",
    "    counties1 = []\n",
    "    counties2 = []\n",
    "\n",
    "    countyStr = countyStr.replace(\"and \", \"\").replace(\"and\", \"\").replace(\"?\", \"\").replace(\".\", \"\").replace(\"Counties\",\"\")\n",
    "    counties1 = countyStr.split(\";\")\n",
    "    for name in counties1:\n",
    "        name = name.strip();\n",
    "        if \"sf\" in name:\n",
    "            name = \"san francisco\"\n",
    "        if \"mateo santa\" in name:\n",
    "            counties2.append(\"san mateo county\")\n",
    "            counties2.append(\"santa clara county\")\n",
    "            continue\n",
    "        if \"mateo san fr\" in name:\n",
    "            counties2.append(\"san mateo county\")\n",
    "            counties2.append(\"san francisco county\")\n",
    "            continue;\n",
    "        if len(name) > 0 and \"county\" not in name and not (name.startswith(\"all\") or name.startswith(\"9\")):\n",
    "            name += \" county\"\n",
    "            counties2.append(name)\n",
    "\n",
    "    for eachName in counties2:\n",
    "        if eachName:\n",
    "            if eachName in countiesDict:\n",
    "                countiesDict[eachName] += 1\n",
    "            else:\n",
    "                countiesDict[eachName] = 1\n",
    "                 \n",
    "            if eachName in areasOfInterestByCounty:\n",
    "                incrementAreasOfInterestCounts(areasOfInterestByCounty[eachName], row)\n",
    "            else:\n",
    "                areasOfInterestByCounty[eachName] = initialize(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def append(to,from_, at):\n",
    "    to.append(from_[at])\n",
    "\n",
    "def appendToClassificationVectors(row):\n",
    "    append(proposedProjectsTitles, row, 0)\n",
    "    append(proposedProjectsAbstracts, row, 1)\n",
    "    append(proposedProjectsLocationDescriptions, row, 6)\n",
    "    append(proposedProjectsProjectTypeDescription, row, 7)\n",
    "    append(proposedProjectsDetailedDescription, row, 8)\n",
    "    append(proposedProjectsProjectNeed, row, 9)\n",
    "    append(proposedProjectsCriticalImpacts, row, 10)\n",
    "    append(proposedProjectsBenefits, row, 11)\n",
    "    \n",
    "    curEssence = row[0] + \" \" + row[1] + \" \" + row[6] + \" \" + row[7] + \" \" + row[8] + \" \" + row[9] + \" \"\n",
    "    curEssence += row[10] + \" \" + row[11]\n",
    "    essence.append(curEssence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def execute():\n",
    "    next(dbreader) # Skip header\n",
    "    \n",
    "    for row in dbreader:\n",
    "        appendCounties(row)\n",
    "        appendBitvectors(row)\n",
    "        fillSponsorsDict(row)\n",
    "        appendToClassificationVectors(row)\n",
    "        \n",
    "execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wr.iplotPlotPie(countiesDict, \"Counties of Proposed Projects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'https://plot.ly/~jlikhuva/96'"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "def createTrace(yvals, i):\n",
    "    trace = {\n",
    "      'x': areasOfInterestByCounty.keys(),\n",
    "      'y': yvals,\n",
    "      'name': areasOfInterest[i],\n",
    "      'type': 'bar'\n",
    "    };\n",
    "    return trace\n",
    "    \n",
    "def gatherValuesAt(index):\n",
    "    ls = []\n",
    "    for _, val in areasOfInterestByCounty.iteritems():\n",
    "        ls.append(val[index])\n",
    "#     print ls\n",
    "    return ls\n",
    "\n",
    "cdata = []\n",
    "for i in range(0, len(areasOfInterest)):\n",
    "    yvals = gatherValuesAt(i)\n",
    "    cdata.append(createTrace(yvals, i))\n",
    "\n",
    "layout = {\n",
    "  'xaxis': {'title': 'Bay Area Counties'},\n",
    "  'yaxis': {'title': 'Areas of interest'},\n",
    "  'barmode': 'relative',\n",
    "  'title': 'Areas Of Interest By County.',\n",
    " 'orientation' : 'h'\n",
    "};\n",
    "py.plot({'data': cdata, 'layout': layout})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def produceBubbleChart(data, maxN):\n",
    "    n = len(data)\n",
    "    chart = go.Scatter(\n",
    "        x = wr.getRandomIntList(0, maxN, n),\n",
    "        y = wr.getRandomIntList(0, maxN, n),\n",
    "        text = data.keys(),\n",
    "        mode = \"markers\",\n",
    "        marker = dict(\n",
    "            size = data.values(),\n",
    "        ) \n",
    "    )\n",
    "    data = [chart]\n",
    "    py.plot(data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def produceAreasOfInterestCharts():\n",
    "    titles = db.kHeadingNames\n",
    "    data = [go.Bar(\n",
    "            x=[titles[13], titles[14], titles[15], titles[16], titles[17], titles[18], titles[19], titles[20]],\n",
    "            y=[sum(drinkingWater), sum(waterQualityImprovement), sum(waterReuseAndRecycling),\n",
    "               sum(stormwaterImprovements), sum(groundwaterBenefits), sum(infiltration),\n",
    "               sum(habitatProtection), sum(floodProtection)],\n",
    "             marker = dict(\n",
    "                color = \"rgb(193, 7, 184)\"\n",
    "            )\n",
    "    )]\n",
    "    py.plot(data)\n",
    "produceAreasOfInterestCharts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def produceHistogram(data):\n",
    "    data = [go.Bar(\n",
    "            x = data.keys(),\n",
    "            y = data.values(),\n",
    "            marker = dict(\n",
    "                color = \"rgb(30, 188, 75)\"\n",
    "            )\n",
    "        )]\n",
    "    py.plot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# wr.iplotPlotPie(sponsorsOfProposedProjects, \"Sponsors of proposed projects.\")\n",
    "# produceBubbleChart(sponsorsOfProposedProjects, 500)\n",
    "produceHistogram(sponsorsOfProposedProjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readInFundingAmounts(row):\n",
    "    fundedProjectsTitles.append(row[0])\n",
    "    projFundingAmounts.append(row[6])\n",
    "    projTotalCosts.append(row[7])\n",
    "    \n",
    "for row in fprojectsreader:\n",
    "    readInFundingAmounts(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Code in this snippet courtesey of S.O User \n",
    "http://stackoverflow.com/users/3923281/ajcr\n",
    "'''\n",
    "def mean(data):\n",
    "    \"\"\"Return the sample arithmetic mean of data.\"\"\"\n",
    "    n = len(data)\n",
    "    if n < 1:\n",
    "        raise ValueError('mean requires at least one data point')\n",
    "    return sum(data)/n # in Python 2 use sum(data)/float(n)\n",
    "\n",
    "def _ss(data):\n",
    "    \"\"\"Return sum of square deviations of sequence data.\"\"\"\n",
    "    c = mean(data)\n",
    "    ss = sum((x-c)**2 for x in data)\n",
    "    return ss\n",
    "\n",
    "def getStdev(data):\n",
    "    \"\"\"Calculates the population standard deviation.\"\"\"\n",
    "    n = len(data)\n",
    "    if n < 2:\n",
    "        raise ValueError('variance requires at least two data points')\n",
    "    ss = _ss(data)\n",
    "    pvar = ss/n # the population variance\n",
    "    return pvar**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getTrace(x, y, title, color):\n",
    "    return go.Scatter(\n",
    "        x = x,\n",
    "        y = y,\n",
    "        mode = \"markers\",\n",
    "        name = title,\n",
    "        marker = dict(\n",
    "            color = color\n",
    "        )\n",
    "    )\n",
    "\n",
    "def drawFundingCharts():\n",
    "    data1 = [getTrace(fundedProjectsTitles, projFundingAmounts, \"Amount of DWR Funding.\", \"rgb(193, 7, 184)\")]\n",
    "    data2 = [getTrace(fundedProjectsTitles, projTotalCosts, \"Total Project Costs.\",\"rgb(30, 188, 75)\" )]\n",
    "    py.plot(data1)\n",
    "    py.plot(data2)\n",
    "    \n",
    "drawFundingCharts()\n",
    "# print \"Mean Funding is: \" + str(mean(projFundingAmounts))\n",
    "# print \"Standard Deviation of Funding: \" + str(getStdev(projFundingAmounts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The following snippets are deal with the\n",
    "classification of the data that we have.\n",
    "\n",
    "It is based on the Amazing tutorial found at\n",
    "https://github.com/brandomr/document_cluster/blob/master/cluster_analysis.ipynb\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "import mpld3\n",
    "from __future__ import print_function\n",
    "from sklearn import feature_extraction\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Code courtesey of Brandon Rose\n",
    "https://github.com/brandomr\n",
    "'''\n",
    "def tokenizeAndStem(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens\n",
    "\n",
    "def printTopTermsPerCluster(N, referenceDF, modelDF, model, terms):\n",
    "    print(\"Top terms per cluster:\")\n",
    "    print()\n",
    "    #sort cluster centers by proximity to centroid\n",
    "    order_centroids = model.cluster_centers_.argsort()[:, ::-1] \n",
    "    \n",
    "    for i in range(N):\n",
    "        print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "        for ind in order_centroids[i, :6]: #replace 6 with n words per cluster\n",
    "            print(' %s' % referenceDF.ix[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'),\n",
    "                  end=',')\n",
    "        print() #add whitespace\n",
    "        print() #add whitespace\n",
    "    \n",
    "        print(\"Cluster %d titles:\" % i, end='')\n",
    "        for title in modelDF.ix[i]['title'].values.tolist():\n",
    "            print(' %s,' % title, end='')\n",
    "        print() #add whitespace\n",
    "        print() #add whitespace\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_TfidfMatrix_Terms_Dist(textList):\n",
    "    vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenizeAndStem,\n",
    "                                 ngram_range=(1,3)\n",
    "                                )\n",
    "    matrix = vectorizer.fit_transform(textList)\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    distance = 1 - cosine_similarity(matrix)\n",
    "    return matrix , terms, distance  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def runKmeans(matrix, N):\n",
    "    kmeans = KMeans(N)\n",
    "    kmeans.fit(matrix)\n",
    "    return kmeans\n",
    "    \n",
    "def openModel(filename):\n",
    "    try:\n",
    "        model = joblib.load(filename)\n",
    "        return model\n",
    "    except:\n",
    "        print (\"could not open \" + str(filename))\n",
    "        \n",
    "def saveModel(model, name):\n",
    "    try:\n",
    "        joblib.dump(model, name)\n",
    "    except:\n",
    "        print (\"Error saving model under \" + name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createDataFrameFromDict(dict_, indexList):\n",
    "    pandasDF = pd.DataFrame(dict_, index = indexList, # may have bug here.[indexList]\n",
    "                            columns = dict_.keys())\n",
    "    return pandasDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "name is the name you want this model\n",
    "to be saved as\n",
    "\n",
    "byList is expected to be a list of\n",
    "text.\n",
    "\n",
    "structure is a dict describing the structure\n",
    "of the data\n",
    "'''\n",
    "def classify(byList, name, structure, N):\n",
    "    totalvocabStemmed = []\n",
    "    totalvocabTokenized = []\n",
    "    for each in byList:\n",
    "        allwordsStemmed = tokenizeAndStem(each) \n",
    "        totalvocabStemmed.extend(allwordsStemmed)\n",
    "      \n",
    "        allwordsTokenized = tokenize(each)\n",
    "        totalvocabTokenized.extend(allwordsTokenized)\n",
    "    \n",
    "    frameBuilder = {\n",
    "        \"words\" : totalvocabTokenized\n",
    "    }\n",
    "    vocabsDataFrame = createDataFrameFromDict(frameBuilder,\n",
    "                                              totalvocabStemmed)\n",
    "    \n",
    "    tfidfmatrix, terms, dist = get_TfidfMatrix_Terms_Dist(byList)\n",
    "    \n",
    "    # Should check if name already exists.\n",
    "    # If it does, do not re-run the model \n",
    "    # read in the model insead.\n",
    "    # openModel(name)\n",
    "    kmeansModel = runKmeans(tfidfmatrix, N)\n",
    "    # saveModel(name)\n",
    "    structure[\"clusters\"] = kmeansModel.labels_.tolist()\n",
    "    \n",
    "    byListDataFrame = createDataFrameFromDict(structure,\n",
    "                                              kmeansModel.labels_.tolist())\n",
    "    printTopTermsPerCluster(N, vocabsDataFrame, byListDataFrame, kmeansModel, terms)\n",
    "    \n",
    "def runClassification():\n",
    "    structure = {\n",
    "        \"title\" : proposedProjectsTitles,\n",
    "        \"abstract\" : proposedProjectsAbstracts\n",
    "    }\n",
    "    df = classify(essence,\n",
    "             \"classification by title\", \n",
    "             structure, \n",
    "             8)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "runClassification()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
